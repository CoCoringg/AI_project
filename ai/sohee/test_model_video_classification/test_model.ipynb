{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.2.3, however version 22.0.3 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q git+https://github.com/tensorflow/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.8.0-cp37-cp37m-win_amd64.whl (437.9 MB)\n",
      "     -------------------------------------- 437.9/437.9 MB 5.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\yohan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow) (3.19.4)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\yohan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Collecting tensorboard<2.9,>=2.8\n",
      "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "     ---------------------------------------- 5.8/5.8 MB 11.2 MB/s eta 0:00:00\n",
      "Collecting typing-extensions>=3.6.6\n",
      "  Downloading typing_extensions-4.1.1-py3-none-any.whl (26 kB)\n",
      "Collecting gast>=0.2.1\n",
      "  Downloading gast-0.5.3-py3-none-any.whl (19 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.44.0-cp37-cp37m-win_amd64.whl (3.4 MB)\n",
      "     ---------------------------------------- 3.4/3.4 MB 9.9 MB/s eta 0:00:00\n",
      "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
      "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
      "     -------------------------------------- 462.5/462.5 KB 9.6 MB/s eta 0:00:00\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.13.3-cp37-cp37m-win_amd64.whl (34 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading h5py-3.6.0-cp37-cp37m-win_amd64.whl (2.8 MB)\n",
      "     ---------------------------------------- 2.8/2.8 MB 11.7 MB/s eta 0:00:00\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "     ---------------------------------------- 42.6/42.6 KB 2.2 MB/s eta 0:00:00\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     ---------------------------------------- 57.5/57.5 KB ? eta 0:00:00\n",
      "Collecting libclang>=9.0.1\n",
      "  Downloading libclang-13.0.0-py2.py3-none-win_amd64.whl (13.9 MB)\n",
      "     ---------------------------------------- 13.9/13.9 MB 7.7 MB/s eta 0:00:00\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "     ---------------------------------------- 65.5/65.5 KB 3.5 MB/s eta 0:00:00\n",
      "Collecting flatbuffers>=1.12\n",
      "  Downloading flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: absl-py>=0.4.0 in c:\\users\\yohan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow) (1.0.0)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.24.0-cp37-cp37m-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 11.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in c:\\users\\yohan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow) (41.2.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\yohan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow) (1.21.5)\n",
      "Collecting keras<2.9,>=2.8.0rc0\n",
      "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
      "     ---------------------------------------- 1.4/1.4 MB 9.7 MB/s eta 0:00:00\n",
      "Collecting wheel<1.0,>=0.23.0\n",
      "  Using cached wheel-0.37.1-py2.py3-none-any.whl (35 kB)\n",
      "Collecting cached-property\n",
      "  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "     ---------------------------------------- 97.8/97.8 KB 5.8 MB/s eta 0:00:00\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-2.0.3-py3-none-any.whl (289 kB)\n",
      "     -------------------------------------- 289.2/289.2 KB 5.9 MB/s eta 0:00:00\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.6.0-py2.py3-none-any.whl (156 kB)\n",
      "     -------------------------------------- 156.3/156.3 KB 1.9 MB/s eta 0:00:00\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "     -------------------------------------- 781.3/781.3 KB 9.9 MB/s eta 0:00:00\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
      "     ---------------------------------------- 63.1/63.1 KB 3.3 MB/s eta 0:00:00\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.0.0-py3-none-any.whl (9.1 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "     -------------------------------------- 155.3/155.3 KB 4.7 MB/s eta 0:00:00\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata>=4.4\n",
      "  Downloading importlib_metadata-4.11.1-py3-none-any.whl (17 kB)\n",
      "Collecting charset-normalizer~=2.0.0\n",
      "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.3-py3-none-any.whl (61 kB)\n",
      "     ---------------------------------------- 61.2/61.2 KB 3.4 MB/s eta 0:00:00\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n",
      "     -------------------------------------- 149.2/149.2 KB 8.7 MB/s eta 0:00:00\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n",
      "     -------------------------------------- 138.7/138.7 KB 8.6 MB/s eta 0:00:00\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.7.0-py3-none-any.whl (5.3 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "     ---------------------------------------- 77.1/77.1 KB ? eta 0:00:00\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "     ---------------------------------------- 151.5/151.5 KB ? eta 0:00:00\n",
      "Using legacy 'setup.py install' for termcolor, since package 'wheel' is not installed.\n",
      "Installing collected packages: tf-estimator-nightly, termcolor, tensorboard-plugin-wit, pyasn1, libclang, keras, flatbuffers, certifi, cached-property, zipp, wrapt, wheel, werkzeug, urllib3, typing-extensions, tensorflow-io-gcs-filesystem, tensorboard-data-server, rsa, pyasn1-modules, opt-einsum, oauthlib, keras-preprocessing, idna, h5py, grpcio, google-pasta, gast, charset-normalizer, cachetools, requests, importlib-metadata, google-auth, astunparse, requests-oauthlib, markdown, google-auth-oauthlib, tensorboard, tensorflow\n",
      "  Running setup.py install for termcolor: started\n",
      "  Running setup.py install for termcolor: finished with status 'done'\n",
      "Successfully installed astunparse-1.6.3 cached-property-1.5.2 cachetools-5.0.0 certifi-2021.10.8 charset-normalizer-2.0.12 flatbuffers-2.0 gast-0.5.3 google-auth-2.6.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.44.0 h5py-3.6.0 idna-3.3 importlib-metadata-4.11.1 keras-2.8.0 keras-preprocessing-1.1.2 libclang-13.0.0 markdown-3.3.6 oauthlib-3.2.0 opt-einsum-3.3.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.27.1 requests-oauthlib-1.3.1 rsa-4.8 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tensorflow-io-gcs-filesystem-0.24.0 termcolor-1.1.0 tf-estimator-nightly-2.8.0.dev2021122109 typing-extensions-4.1.1 urllib3-1.26.8 werkzeug-2.0.3 wheel-0.37.1 wrapt-1.13.3 zipp-3.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow imutils opencv-python matplotlib imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%wget` not found.\n"
     ]
    }
   ],
   "source": [
    "#없어도 될듯..?\n",
    "%wget -q https://git.io/JGc31 -O ucf101_top5.tar.gz\n",
    "%tar xf ucf101_top5.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_docs.vis import embed\n",
    "from tensorflow import keras\n",
    "from imutils import paths\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "MAX_SEQ_LENGTH = 20\n",
    "NUM_FEATURES = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total videos for training: 20\n",
      "Total videos for testing: 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_name</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16.avi</td>\n",
       "      <td>neck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15.avi</td>\n",
       "      <td>neck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.avi</td>\n",
       "      <td>nose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.avi</td>\n",
       "      <td>nose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.avi</td>\n",
       "      <td>nose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11.avi</td>\n",
       "      <td>neck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12.avi</td>\n",
       "      <td>neck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17.avi</td>\n",
       "      <td>neck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.avi</td>\n",
       "      <td>nose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.avi</td>\n",
       "      <td>nose</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   video_name   tag\n",
       "16     16.avi  neck\n",
       "15     15.avi  neck\n",
       "4       4.avi  nose\n",
       "6       6.avi  nose\n",
       "1       1.avi  nose\n",
       "11     11.avi  neck\n",
       "12     12.avi  neck\n",
       "17     17.avi  neck\n",
       "3       3.avi  nose\n",
       "2       2.avi  nose"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#데이터 확인\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "print(f\"Total videos for training: {len(train_df)}\")\n",
    "print(f\"Total videos for testing: {len(test_df)}\")\n",
    "\n",
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_center_square(frame):\n",
    "    y, x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#비디오 불러와서 전처리\n",
    "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center_square(frame)\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_extractor():\n",
    "    feature_extractor = keras.applications.InceptionV3(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    )\n",
    "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "\n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "\n",
    "feature_extractor = build_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neck', 'nose']\n"
     ]
    }
   ],
   "source": [
    "#카테고리 찾기\n",
    "label_processor = keras.layers.StringLookup(\n",
    "    num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"])\n",
    ")\n",
    "print(label_processor.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.avi'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#추후에 데이터 파일안에 정리\n",
    "sample = [1,2,3,4]\n",
    "var2=[]\n",
    "\n",
    "for i in sample:\n",
    "    var2 = str(sample[i-1])+\".avi\"\n",
    "\n",
    "var2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_all_videos(df, root_dir):\n",
    "    num_samples = len(df)\n",
    "    video_paths = df[\"video_name\"].values.tolist()\n",
    "    print(video_paths)\n",
    "    labels = df[\"tag\"].values\n",
    "    labels = label_processor(labels[..., None]).numpy()\n",
    "\n",
    "    # `frame_masks` and `frame_features` are what we will feed to our sequence model.\n",
    "    # `frame_masks` will contain a bunch of booleans denoting if a timestep is\n",
    "    # masked with padding or not.\n",
    "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n",
    "    frame_features = np.zeros(\n",
    "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "    )\n",
    "\n",
    "    # For each video.\n",
    "    for idx, path in enumerate(video_paths):\n",
    "        # Gather all its frames and add a batch dimension.\n",
    "        frames = load_video(os.path.join(root_dir, path))\n",
    "        frames = frames[None, ...]\n",
    "\n",
    "        # Initialize placeholders to store the masks and features of the current video.\n",
    "        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "        temp_frame_features = np.zeros(\n",
    "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "        )\n",
    "\n",
    "        # Extract features from the frames of the current video.\n",
    "        for i, batch in enumerate(frames):\n",
    "            video_length = batch.shape[0]\n",
    "            length = min(MAX_SEQ_LENGTH, video_length)\n",
    "            for j in range(length):\n",
    "                temp_frame_features[i, j, :] = feature_extractor.predict(\n",
    "                    batch[None, j, :]\n",
    "                )\n",
    "            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "        frame_features[idx,] = temp_frame_features.squeeze()\n",
    "        frame_masks[idx,] = temp_frame_mask.squeeze()\n",
    "\n",
    "    return (frame_features, frame_masks), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.avi', '1.avi', '2.avi', '3.avi', '4.avi', '5.avi', '6.avi', '7.avi', '8.avi', '9.avi', '10.avi', '11.avi', '12.avi', '13.avi', '14.avi', '15.avi', '16.avi', '17.avi', '18.avi', '19.avi']\n",
      "['20.avi', '21.avi', '22.avi', '23.avi', '24.avi', '25.avi']\n",
      "Frame features in train set: (20, 20, 2048)\n",
      "Frame masks in train set: (20, 20)\n"
     ]
    }
   ],
   "source": [
    "train_data, train_labels = prepare_all_videos(train_df, \"train\")\n",
    "test_data, test_labels = prepare_all_videos(test_df, \"test\")\n",
    "\n",
    "print(f\"Frame features in train set: {train_data[0].shape}\")\n",
    "print(f\"Frame masks in train set: {train_data[1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 7s 7s/step - loss: 0.6931 - accuracy: 0.2857 - val_loss: 0.6941 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.6927 - accuracy: 0.7143 - val_loss: 0.6951 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.6923 - accuracy: 0.7143 - val_loss: 0.6962 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.6919 - accuracy: 0.7143 - val_loss: 0.6972 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.6914 - accuracy: 0.7143 - val_loss: 0.6982 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.6910 - accuracy: 0.7143 - val_loss: 0.6992 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.6906 - accuracy: 0.7143 - val_loss: 0.7002 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.6902 - accuracy: 0.7143 - val_loss: 0.7012 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.6898 - accuracy: 0.7143 - val_loss: 0.7022 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.6893 - accuracy: 0.7143 - val_loss: 0.7032 - val_accuracy: 0.0000e+00\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6932 - accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "def get_sequence_model():\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "    frame_features_input = tf.keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
    "    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "\n",
    "    # Refer to the following tutorial to understand the significance of using `mask`:\n",
    "    # https://keras.io/api/layers/recurrent_layers/gru/\n",
    "    \n",
    "    x = tf.keras.layers.GRU(16, return_sequences=True)(\n",
    "        frame_features_input, mask=mask_input\n",
    "    )\n",
    "    x = tf.keras.layers.GRU(8)(x)\n",
    "    x = tf.keras.layers.Dropout(0.4)(x)\n",
    "    x = tf.keras.layers.Dense(8, activation=\"relu\")(x)\n",
    "    output = tf.keras.layers.Dense(len(class_vocab), activation=\"softmax\")(x)\n",
    "\n",
    "    rnn_model = tf.keras.Model([frame_features_input, mask_input], output)\n",
    "\n",
    "    rnn_model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return rnn_model\n",
    "\n",
    "\n",
    "# Utility for running experiments.\n",
    "def run_experiment():\n",
    "    filepath = \"/train_data_result\"\n",
    "    #checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, save_weights_only=False, save_best_only=True, verbose=1)\n",
    "    #cb_earlystop = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",mode=\"auto\",verbose=1,patience=2)\n",
    "\n",
    "    seq_model = get_sequence_model()\n",
    "    history = seq_model.fit(\n",
    "        [train_data[0], train_data[1]],\n",
    "        train_labels,\n",
    "        validation_split=0.3,\n",
    "        epochs=EPOCHS,\n",
    "        #callbacks = [cb_earlystop, checkpoint],\n",
    "    )\n",
    "#test accuracy 확인하기\n",
    "    #seq_model.load_weights(filepath)\n",
    "    _, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)\n",
    "    #print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history, seq_model \n",
    "\n",
    "    \n",
    "\n",
    "_, sequence_model = run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a725ad1fa9ec9c0aa245097d86972e9adb01176843dc8ad807ba5401f8ffe960"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
