{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MediaPipe\n",
    "\n",
    "- holistic 객체의 \n",
    "- face_landmarks = 얼굴 랜드마크\n",
    "- left_hand_landmarks = 왼쪽 손 랜드마크\n",
    "- right_hand_landmarks = 오른쪽 손 랜드마크\n",
    "- pose_landmarks = pose 랜드마크\n",
    "- [mediapipe-documentation]https://google.github.io/mediapipe/getting_started/python.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1. Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "# train, test 데이터 분할\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "'''\n",
    "클래스 벡터(정수)를 이진 클래스 행렬로 변환한다.\n",
    "ex) 클래스 (0, 1, 2)인 label 데이터가 있다고 가정.\n",
    "'0' 클래스인 경우 => [1, 0 ,0]\n",
    "'1' 클래스인 경우 => [0, 1, 0]\n",
    "'2' 클래스인 경우 => [0, 0, 1]\n",
    "'''\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Keypoints using MP Holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities\n",
    "\n",
    "# mediapipe 감지 함수\n",
    "def mediapipe_detection(image,model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results\n",
    "\n",
    "\n",
    "# landmark 특징점 그리는 함수\n",
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION) # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections\n",
    "\n",
    "\n",
    "# landmark 특징점의 선 굵기나, 색상을 변경하는 함수.\n",
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION,\n",
    "                                # 색상 변경. 선의 굵기나, 색상을 변경한다.\n",
    "                                mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                                mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                            ) \n",
    "\n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                                # 색상 변경. 선의 굵기나, 색상을 변경한다.\n",
    "                                mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                                mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "    )\n",
    "\n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                                # 색상 변경. 선의 굵기나, 색상을 변경한다.\n",
    "                                mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "                                mp_drawing.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2)\n",
    "    ) \n",
    "\n",
    "    # Draw right hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                                # 색상 변경. 선의 굵기나, 색상을 변경한다.\n",
    "                                mp_drawing.DrawingSpec(color=(245 ,117, 66), thickness=2, circle_radius=4),\n",
    "                                mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2)\n",
    "    ) \n",
    "\n",
    "\n",
    "# keypoints 추출 함수\n",
    "def extract_keypoints(results):\n",
    "    # pose의 landmark 배열을 일차원으로 펴서 반환한다. pose landmark가 없을 시 동일한 shape를 가진 영행렬을 반환한다.\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33 * 4) # member : x,y,z,visibility\n",
    "\n",
    "    # face의 landmark 배열을 일차원으로 펴서 반환. face landmark가 없을 시 동일한 shape를 가진 영행렬을 반환한다.\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468 * 3) # member : x,y,z\n",
    "\n",
    "    # 왼손 랜드마크 배열 left_hand_landmarks가 없을 경우에는 동일한 shape의 영행렬을 반환한다.\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3) # member : x,y,z\n",
    "\n",
    "    # 오른손 랜드마크 배열 left_hand_landmarks가 없을 경우에는 동일한 shape의 영행렬을 반환한다.\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3) # member : x,y,z\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영상 테스트\n",
    "file_num = '0'\n",
    "ear_format = f'train_data/ear/{file_num}.avi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    비디오 프레임 측정 \n",
    "'''\n",
    "cap = cv2.VideoCapture(ear_format)\n",
    "length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 웹캠을 실행시켜 landmark를 감지하는 블록.\n",
    "\n",
    "cap = cv2.VideoCapture(ear_format)\n",
    "length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) # Frame 길이 측정. \n",
    "print(f'length : {length}')\n",
    "frame_jump = round(length / 30)\n",
    "print(frame_jump)\n",
    "\n",
    "frame_cnt = 0 \n",
    "# frame 측정.\n",
    "saved_frames = 0 \n",
    "prev = None\n",
    "# Set mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "    \n",
    "        # frame을 읽는다.\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # 읽어올 거 없으면 알아서 끄세요.\n",
    "        if not ret:\n",
    "            while saved_frames < 30:\n",
    "                print(f\"new frame {saved_frames} added\")\n",
    "                saved_frames += 1\n",
    "            break\n",
    "        \n",
    "        frame_cnt += 1\n",
    "        prev = frame\n",
    "        \n",
    "        if saved_frames < 30 and frame_cnt % frame_jump == 0:\n",
    "            print(f\"saved {saved_frames}\")\n",
    "            saved_frames += 1\n",
    "\n",
    "            # Make detections   \n",
    "            image, results = mediapipe_detection(frame, holistic)\n",
    "        \n",
    "            # Draw landmarks\n",
    "            draw_styled_landmarks(image, results)\n",
    "\n",
    "            # video show\n",
    "            cv2.imshow(\"WebCam\", image)\n",
    "\n",
    "        # 끄고싶을때 사용하는 키\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "            \n",
    "        \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results.face_landmarks.landmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_landmarks(prev, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(prev, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Keypoint Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.pose_landmarks.landmark[0].visibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = []\n",
    "for res in results.pose_landmarks.landmark:\n",
    "    # randmark의 좌표 추출\n",
    "    test = np.array([res.x, res.y, res.z, res.visibility])\n",
    "    pose.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pose의 landmark 배열을 일차원으로 펴서 반환한다. pose landmark가 없을 시 동일한 shape를 가진 영행렬을 반환한다.\n",
    "pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(132)\n",
    "\n",
    "# face의 landmark 배열을 일차원으로 펴서 반환. face landmark가 없을 시 동일한 shape를 가진 영행렬을 반환한다.\n",
    "face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(1404)\n",
    "\n",
    "# 왼손 랜드마크 배열 left_hand_landmarks가 없을 경우에는 동일한 shape의 영행렬을 반환한다.\n",
    "lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "\n",
    "# 오른손 랜드마크 배열 left_hand_landmarks가 없을 경우에는 동일한 shape의 영행렬을 반환한다.\n",
    "rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'right hand shape : {rh.shape}')\n",
    "print(f'lefh hand shape : {lh.shape}')\n",
    "print(f'pose shape : {pose.shape}')\n",
    "print(f'face shape : {face.shape}')\n",
    "print(f'all mediapipe shape : {126 + 132 + 1404}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# face의 landmark에서 x, y, z의 총수를 곱한 것이 face_lanemark를 1차원 행렬로 폈을때의 shape와 같다.\n",
    "face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() \\\n",
    "if results.face_landmarks \\\n",
    "else np.zeros(1404)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skelleton의 keypoints 추출\n",
    "extract_keypoints(results).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_keypoints(results)[:-10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setup Folders for Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = os.listdir(\"train_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추출할 numpy array 타입의 데이터 PATH\n",
    "DATA_PATH = os.path.join(\"Multi_Pose_Data\")\n",
    "\n",
    "# 특정 행동 들을 감지하려는 작업 (hello, thanks, iloveyou)\n",
    "actions = np.array(actions)\n",
    "\n",
    "# 비디오 숫자.\n",
    "no_sequences = 240\n",
    "\n",
    "# 비디오 내의 전체 프레임\n",
    "sequence_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        try:\n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test = extract_keypoints(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각프레임 데이터를 numpy 데이터 형태로 저장.\n",
    "'''\n",
    "현재 hello 단어의 프레임이 0부터 ~ 29 까지 인데\n",
    "예시로 hello의 각 프레임마다 numpy 형태로 데이터가 저장되어서\n",
    "해당 단어를 인식한다.    \n",
    "'''\n",
    "np.save('0', result_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장한 numpy 형태의 배열 파일을 불러온다.\n",
    "np_result = np.load('0.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Collect Keypoint Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = os.path.join(\"train_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_folder = os.listdir(TRAIN_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_file = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video path\n",
    "for word in word_folder:\n",
    "    \n",
    "    array = []\n",
    "    test = os.path.join(TRAIN_DATA_PATH,word)\n",
    "    test_file = os.listdir(test)\n",
    "    for t in test_file:\n",
    "        test_x = os.path.join(test,t)\n",
    "        array.append(test_x)\n",
    "    video_file[word] = array\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 행동 데이터를 수집해서 각 시퀀스를 numpy 배열 형태로 저장한다.\n",
    "\n",
    "# Loop through actions ex) ['ear', 'snow', 'leg']\n",
    "for action in actions:\n",
    "    \n",
    "    for sequence in range(no_sequences): # 비디오 개수 만큼 루프\n",
    "        cap = cv2.VideoCapture(video_file[action][sequence])\n",
    "        length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) # Frame 길이 측정. \n",
    "        frame_jump = round(length / 30)\n",
    "\n",
    "        frame_cnt = 0 \n",
    "        # frame 측정.\n",
    "        saved_frames = 0 # (saved_frames).npy 파일로 저장.\n",
    "        prev = None\n",
    "\n",
    "        # Set mediapipe model\n",
    "        with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "\n",
    "            # Loop through sequences aka videos ex) no_sequences == 20 비디오 개수\n",
    "                \n",
    "            print(f'sequence : {sequence}')\n",
    "            # Loop through video length aka sequence length\n",
    "            '''\n",
    "                해야 되는 것\n",
    "                원래 video frame을 잘라서. \n",
    "                30 프레임 형태로 만들어줘야함.\n",
    "            '''\n",
    "            while cap.isOpened():\n",
    "                \n",
    "                # frame을 읽는다.\n",
    "                ret, frame = cap.read()\n",
    "\n",
    "                \n",
    "                # 읽어올 거 없으면 알아서 끄세요.\n",
    "                if not ret:\n",
    "                    break\n",
    "                \n",
    "                frame_cnt += 1\n",
    "                \n",
    "                # 4초 아니면 3초 아니면 2초 당 frame을 추출하는 로직. frame_jump가 영상 길이.\n",
    "                if saved_frames < 30 and frame_cnt % frame_jump == 0:\n",
    "\n",
    "                    # Make detections\n",
    "                    image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "                    prev = results\n",
    "\n",
    "                    # Draw landmarks\n",
    "                    draw_styled_landmarks(image, results)\n",
    "\n",
    "                    '''\n",
    "                        내보낼 keypoints 들\n",
    "                        1. 각 프레임 마다 keypoints를 추출한다.\n",
    "                        2. 각 프레임 마다 DATA_PATH에 맞게 해당 keypoints를 numpy 데이터 형태로 npy_path에 저장한다.\n",
    "                        3. 해당 numpy 데이터 형태를 저장한다.\n",
    "                    '''\n",
    "                    keypoints = extract_keypoints(results)\n",
    "                    npy_path = os.path.join(DATA_PATH, action, str(sequence), str(saved_frames))\n",
    "\n",
    "                    print(f'npy_path : {npy_path}')\n",
    "\n",
    "                    np.save(npy_path, keypoints)\n",
    "\n",
    "                    print(f\"saved {saved_frames}\")\n",
    "                    saved_frames += 1\n",
    "                    \n",
    "                    # 스크린에 보여준다.\n",
    "                    # cv2.imshow('OpenCV Feed', image)\n",
    "                    \n",
    "                \n",
    "            while saved_frames < 30:\n",
    "                keypoints = extract_keypoints(prev)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(saved_frames))\n",
    "                print(f\"new frame {saved_frames} added\")\n",
    "                np.save(npy_path, keypoints)\n",
    "                saved_frames += 1\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Preprocess Data and Create Labels and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actions 단어들에 labeling\n",
    "label_map = {label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "단어마다 30개의 프레임이 존재.\n",
    "단어 * 30 개의 넘파이 배열에서\n",
    "keypoints를 나타내는 총 1662개의 특정 값들이 필요함.\n",
    "'''\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    numpy 배열 불러오기\n",
    "    \n",
    "'''\n",
    "\n",
    "# sequences, labels 배열\n",
    "sequences, labels = [], [] \n",
    "\n",
    "for action in actions:\n",
    "    # 각 프레임 마다.(이 예제에서는 30)\n",
    "    for sequence in range(no_sequences):\n",
    "        window = []\n",
    "\n",
    "        # sequence의 길이마다(이 예제에서는 30) \n",
    "        for frame_num in range(sequence_length):\n",
    "\n",
    "            '''\n",
    "            각 sequence의 numpy 형태 배열을 불러온다.\n",
    "            ex) hello > 0 > 0.npy\n",
    "            window.append(hello > 0 > 0.npy)\n",
    "            '''\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "            # 해당 numpy 배열을 window 배열에 추가한다.\n",
    "            window.append(res)\n",
    "\n",
    "        '''\n",
    "            각 frame의 배열 모음을 추가한다.\n",
    "            ex) hello > 0(numpy 배열 모음 0 ~ 30.npy)\n",
    "\n",
    "            frame 배열에 라벨링을 추가해준다.\n",
    "            ex) sequences = [\n",
    "                [hello.0], [hello.1], [hello.2] ....\n",
    "                [thanks.0], [thanks.1], [thanks.2] ...\n",
    "            ]\n",
    "            \n",
    "            labels_map = { 'hello': 0, 'thanks': 1, 'iloveyou': 2}\n",
    "            labels =[\n",
    "                0, 0, 0, 0, 0 ....\n",
    "                1, 1, 1, 1, 1 ....\n",
    "                2, 2, 2, 2, 2 ....\n",
    "            ]\n",
    "        '''\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. sequences의 배열에는 각 단어마다 30개의 sequence\n",
    "총 90개의 sequence를 가진다.\n",
    "\n",
    "2. 각 시퀀스(프레임)마다 30번의 keypoints 추출 작업을 거친다.\n",
    "3. 총 1662개의 key 포인트를 가지는 numpy 배열을 가진다.\n",
    "sequences.shape = (90, 30, 1662)\n",
    "\n",
    "labels_map = { 'hello': 0, 'thanks': 1, 'iloveyou': 2}\n",
    "            labels =[\n",
    "                0, 0, 0, 0, 0 ....\n",
    "                1, 1, 1, 1, 1 ....\n",
    "                2, 2, 2, 2, 2 ....\n",
    "            ]\n",
    "'''\n",
    "print(f'sequences의 shape : {np.array(sequences).shape}')\n",
    "print(f'sequences의 길이 : {len(sequences)}')\n",
    "print(f'labels의 shape : {np.array(labels).shape}')\n",
    "print(f'labels의 길이 : {len(labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'X_train : {X_train.shape}')\n",
    "print(f'train_label : {y_train.shape}')\n",
    "print(f'X_test : {X_test.shape}')\n",
    "print(f'test_label : {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Build and Train LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard # 로깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    LSTM Neural network\n",
    "'''\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,1662))) # (frame , keypoints)\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "'''\n",
    "    One-Hot-Encoding\n",
    "    ex) res = [0.2, 0.7, 0.1]\n",
    "    가장 확률 높은 인덱스 번호를 추출 \n",
    "        => (모델이 예측한 확률이 가장 높은 인덱스 추출.)\n",
    "        => 예측. (방금한 행동은 아마 1번 일거야)\n",
    "    np.argmax(res) == 1 \n",
    "    actions[np.argmax(res)] == 'thanks'\n",
    "    너가 방금한 행동은 'thanks'야\n",
    "'''\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = [.2, 0.7, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[np.argmax(res)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "다중 클래스 분류 시 일반적인 손실함수 : categorical_crossentropy\n",
    "다중 클래스 분류 시 일반적인 정확도 측정 함수 : categorical_accuracy\n",
    "'''\n",
    "model.compile(\n",
    "    optimizer='adam', \n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['categorical_accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Logs\\train 폴더에 들어간뒤\n",
    "    tensorboard --logdir=. 을 입력하면 \n",
    "    Tensorbaord화면이 나온다.\n",
    "    ex) \n",
    "    1step. cd suhwa_dataset\\Logs\\train\n",
    "    2step. tesorboard --logdir=.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=2000, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측.\n",
    "actions[np.argmax(res[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[np.argmax(y_test[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 저장\n",
    "model.save('multi_pose.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 불러오기 load_model\n",
    "'''\n",
    "    1. 모델 재구축 실행 => model build\n",
    "    2. 모델 컴파일 => model compile\n",
    "    3. 모델 불러오기 => lodad_weights\n",
    "'''\n",
    "model.load_weights('multi_pose.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluation using Confusion Matrix and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Confusion Matrix를 통한 정확도 검증\n",
    "'''\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_train, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_confusion_matrix(ytrue,yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Test in Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [(245,117,16), (117,245,16), (16,117,245)]\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,18))\n",
    "plt.imshow(prob_viz(res, actions, image, colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_num = '0177'\n",
    "snow_format = f'data/snow/KETI_SL_000000{file_num}.avi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_num = \"0150\"\n",
    "file_format = f'train_data/ear/1.avi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. New detection variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "threshold = 0.6\n",
    "\n",
    "for i in range(20):\n",
    "    cap = cv2.VideoCapture(f'train_data/ear/{i}.avi')\n",
    "    # Set mediapipe model \n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        while cap.isOpened():\n",
    "\n",
    "            # Read feed\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Make detections\n",
    "            image, results = mediapipe_detection(frame, holistic)\n",
    "            print(results)\n",
    "            \n",
    "            # Draw landmarks\n",
    "            draw_styled_landmarks(image, results)\n",
    "            \n",
    "            # 2. Prediction logic\n",
    "            keypoints = extract_keypoints(results)\n",
    "    #         sequence.insert(0,keypoints)\n",
    "    #         sequence = sequence[:30]\n",
    "            sequence.append(keypoints)\n",
    "            sequence = sequence[-30:]\n",
    "            \n",
    "            if len(sequence) == 30:\n",
    "                # numpy 배열 차원 추가\n",
    "                # 이 예제에서는 (30, 1662) => (1, 30, 1662)\n",
    "                res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "                print(f'predict result : {actions[np.argmax(res)]}')\n",
    "                print(f'res : {res}')\n",
    "                \n",
    "                \n",
    "                #3. Viz logic\n",
    "                if res[np.argmax(res)] > threshold: \n",
    "                    if len(sentence) > 0: \n",
    "                        if actions[np.argmax(res)] != sentence[-1]:\n",
    "                            sentence.append(actions[np.argmax(res)])\n",
    "                    else:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "                if len(sentence) > 5: \n",
    "                    sentence = sentence[-5:]\n",
    "\n",
    "                # Viz probabilities\n",
    "                # image = prob_viz(res, actions, image, colors) # color를 class 개수만큼 준비해야 함.\n",
    "                \n",
    "            # cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "            # cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "            #             cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            \n",
    "            # Show to screen\n",
    "            # cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "            # Break gracefully\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'predict result : {actions[np.argmax(res)]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[np.argmax(res)] > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 예측 규격에 맞게 데이터를 캡슐화\n",
    "np.expand_dims(X_test[0].shape, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(np.expand_dims(X_test[0], axis=0))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2bd04dc09fab68ca3ef149792dfe3505936e6301cdd003bc123cc71dc1164e1a"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
